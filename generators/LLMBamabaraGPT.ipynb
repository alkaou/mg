{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7f41ac76-b261-4938-8d9b-b8fa63b62116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import tensorflow as tf, keras\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "import numpy as np\n",
    "from tensorflow.keras.saving import register_keras_serializable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6263da5c-1dce-4b4d-8fa5-192be2d26ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_array = [\n",
    "    \"datasets_bambara/poemes/poeme_br_1.txt\",\n",
    "    \"datasets_bambara/poemes/poeme_br_2.txt\",\n",
    "    \"datasets_bambara/poemes/poeme_br_3.txt\",\n",
    "    \"datasets_bambara/poemes/poeme_br_4.txt\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c4b886f1-6be3-48c2-b569-f695b9e01094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "vocab_size = 20000\n",
    "batch_size = 16  # how many independent sequences will we process in parallel?\n",
    "block_size = 32  # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "\n",
    "Epochs = max_iters // eval_interval\n",
    "\n",
    "learning_rate = 1e-9\n",
    "eval_iters = 200\n",
    "n_embd = 1064\n",
    "n_head = 10\n",
    "n_layer = 6\n",
    "dropout = 0.0\n",
    "# ------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "8dbaafb4-3b93-46bc-8382-b4c26235a5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\n",
    "# Read the text file\n",
    "for dtext in data_array:\n",
    "    with open(dtext, \"r\", encoding=\"utf-8\") as f:\n",
    "        text += f\"{f.read()}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "e4bc44f6-16a5-4a50-875a-26db56fd2e3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76129"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "d3318538-de0e-43de-bff3-4b8bbe7525f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tokenizers import Tokenizer, pre_tokenizers, decoders, trainers, processors\n",
    "from tokenizers import models as t_model\n",
    "# from data_list import DATA_LIST\n",
    "\n",
    "tokenizer_path = \"tokenizer.json\"\n",
    "tokenizer = Tokenizer(t_model.BPE())\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=True)\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "tokenizer.post_processor = processors.ByteLevel(trim_offsets=True)\n",
    "trainer = trainers.BpeTrainer(\n",
    "    vocab_size=20000,\n",
    "    min_frequency=2,\n",
    "    initial_alphabet=pre_tokenizers.ByteLevel.alphabet()\n",
    ")\n",
    "\n",
    "\n",
    "class SuperTokenizer():\n",
    "\n",
    "    # {files_path_array} doit être un tableau. EX: [\"1.txt\", \"2.txt\", \"3.txt\"]\n",
    "    @staticmethod\n",
    "    def fit(files_path_array):\n",
    "        # print(files_path_array)\n",
    "        tokenizer.train(files_path_array, trainer=trainer)\n",
    "        tokenizer.save(tokenizer_path, pretty=True)\n",
    "        return True\n",
    "    \n",
    "    @staticmethod\n",
    "    def loader_tokenizer_from_json():\n",
    "        if os.path.exists(tokenizer_path):\n",
    "            loader_tokenizer = Tokenizer.from_file(tokenizer_path)\n",
    "        else:\n",
    "            SuperTokenizer.fit(filenames)\n",
    "            loader_tokenizer = Tokenizer.from_file(tokenizer_path)\n",
    "        return loader_tokenizer\n",
    "    \n",
    "    def encode(text):\n",
    "        loader_tokenizer = SuperTokenizer.loader_tokenizer_from_json()\n",
    "        encoded = loader_tokenizer.encode(text)\n",
    "        return encoded.ids\n",
    "    \n",
    "    def decode(tokens):\n",
    "        loader_tokenizer = SuperTokenizer.loader_tokenizer_from_json()\n",
    "        decoded = loader_tokenizer.decode(tokens)\n",
    "        if decoded[0] == 220 and decoded[-1] == 220:\n",
    "            decoded = decoded[1:-1]\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "9e3f2293-e0c8-47c2-ad0c-b8d4388212b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique characters\n",
    "# chars = sorted(list(set(text)))\n",
    "# vocab_size = len(chars)\n",
    "# # chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "e21f45af-0c71-4cca-8fdf-184615cb51bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a mapping from characters to integers\n",
    "# stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "# itos = {i: ch for i, ch in enumerate(chars)}\n",
    "# encode = lambda s: [\n",
    "#     stoi[c] for c in s\n",
    "# ]  # encoder: take a string, output a list of integers\n",
    "# decode = lambda l: \"\".join(\n",
    "#     [itos[i] for i in l]\n",
    "# )  # decoder: take a list of integers, output a string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "aecbe1c5-dc5b-42fd-bdda-7e6e39e39335",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([314, 485, 365, ..., 605,  19,  15], dtype=int64)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train and test splits\n",
    "data = np.array(SuperTokenizer.encode(text), dtype=np.int64)\n",
    "n = int(0.9 * len(data))  # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "9681dc02-0e11-4ca7-b8e5-b7aa0f055531",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([314, 485, 365, ..., 605,  19,  15], dtype=int64)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "90cb676a-abd5-40f0-9819-6388a8025c0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 314,  485,  365, ...,   42,  372, 1375], dtype=int64)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "736abe68-fa5b-4dd9-9080-20ea24b30647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([331,   0, 974, 119, 198], dtype=int64)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "fe2b2304-a8ec-4e32-8391-ce7dd8fb79f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading\n",
    "def get_batch(split):\n",
    "    # Generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    ix = np.random.randint(0, len(data) - block_size, batch_size)\n",
    "    x = np.stack([data[i : i + block_size] for i in ix])\n",
    "    y = np.stack([data[i + 1 : i + block_size + 1] for i in ix])\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "7d764339-e5ea-4f6c-8400-b3bd819079e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare train/val dataset\n",
    "def train_data_generator():\n",
    "    while True:\n",
    "        yield get_batch(\"train\")\n",
    "\n",
    "\n",
    "def val_data_generator():\n",
    "    while True:\n",
    "        yield get_batch(\"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "ba1fe39d-4fe2-49b3-a28e-028e798bbb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_generator = tf.data.Dataset.from_generator(\n",
    "    train_data_generator,\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(batch_size, block_size), dtype=tf.int64),\n",
    "        tf.TensorSpec(shape=(batch_size, block_size), dtype=tf.int64),\n",
    "    ),\n",
    ")\n",
    "\n",
    "val_data_generator = tf.data.Dataset.from_generator(\n",
    "    val_data_generator,\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(batch_size, block_size), dtype=tf.int64),\n",
    "        tf.TensorSpec(shape=(batch_size, block_size), dtype=tf.int64),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "eb81b2d9-1cd3-48d8-bc15-3b2060deb935",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(layers.Layer):\n",
    "    \"\"\"A simple linear layer followed by a non-linearity\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, dropout=0.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.n_embd = n_embd\n",
    "        self.dropout = dropout\n",
    "        self.net = models.Sequential(\n",
    "            [\n",
    "                layers.Dense(4 * n_embd, activation=\"relu\"),\n",
    "                layers.Dense(n_embd),\n",
    "                layers.Dropout(dropout),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def call(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"n_embd\": self.n_embd,\n",
    "            \"dropout\": self.dropout,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "eb9da14f-a251-4fcc-a61a-1ea2f3031b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(layers.Layer):\n",
    "    \"\"\"Transformer block: communication followed by computation\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head, dropout=0.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.n_embd = n_embd\n",
    "        self.n_head = n_head\n",
    "        self.dropout = dropout\n",
    "        self.sa = layers.MultiHeadAttention(\n",
    "            num_heads=n_head, key_dim=n_embd // n_head, dropout=dropout\n",
    "        )\n",
    "        self.ffwd = FeedForward(n_embd, dropout)\n",
    "        self.ln1 = layers.LayerNormalization()\n",
    "        self.ln2 = layers.LayerNormalization()\n",
    "\n",
    "    def call(self, x):\n",
    "        attn_output = self.sa(\n",
    "            self.ln1(x), self.ln1(x), use_causal_mask=True\n",
    "        )  # use causal mask to ensure each token can only see previous tokens\n",
    "        x = x + attn_output\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"n_embd\": self.n_embd,\n",
    "            \"n_head\": self.n_head,\n",
    "            \"dropout\": self.dropout,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "4cf8cca1-6637-4304-9ab2-6ed5cbd7ffb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bigram Language Model\n",
    "@register_keras_serializable()\n",
    "class BigramLanguageModel(keras.Model):\n",
    "    def __init__(self, vocab_size, n_embd, block_size, n_head, n_layer, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.token_embedding_table = layers.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = layers.Embedding(block_size, n_embd)\n",
    "        self.blocks = [Block(n_embd, n_head) for _ in range(n_layer)]\n",
    "        self.ln_f = layers.LayerNormalization()\n",
    "        self.lm_head = layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx)  # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(\n",
    "            tf.range(T)[tf.newaxis, :]\n",
    "        )  # initially (T,C) adding new axis and get # (1,T,C)\n",
    "        x = tok_emb + pos_emb  # (B,T,C)\n",
    "        for block in self.blocks:  # (B,T,C)\n",
    "            x = block(x)\n",
    "        x = self.ln_f(x)  # (B,T,C)\n",
    "        logits = self.lm_head(x)  # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            return logits, None\n",
    "\n",
    "        logits_flat = tf.reshape(logits, [-1, logits.shape[-1]])\n",
    "        targets_flat = tf.reshape(targets, [-1])\n",
    "        loss = keras.losses.sparse_categorical_crossentropy(\n",
    "            targets_flat, logits_flat, from_logits=True\n",
    "        )\n",
    "        return logits, tf.reduce_mean(loss)\n",
    "\n",
    "    def train_step(self, data):\n",
    "        x, y = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits, loss = self(x, y)\n",
    "        grads = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        x, y = data\n",
    "        logits, loss = self(x, y)\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            idx_next = tf.random.categorical(logits, num_samples=1)\n",
    "            idx = tf.concat([idx, idx_next], axis=1)\n",
    "        return idx\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"vocab_size\": self.token_embedding_table.input_dim,\n",
    "            \"n_embd\": self.token_embedding_table.output_dim,\n",
    "            \"block_size\": self.position_embedding_table.input_dim,\n",
    "            \"n_head\": self.blocks[0].sa.num_heads,\n",
    "            \"n_layer\": len(self.blocks),\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "b875dd4f-37de-4857-a619-b414c85c38f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Alkaou\\Python Projects\\Br_GPT\\venv\\lib\\site-packages\\keras\\src\\layers\\layer.py:361: UserWarning: `build()` was called on layer 'bigram_language_model_6', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model train and plotting loss curves\n",
    "model = BigramLanguageModel(vocab_size=vocab_size, n_embd=n_embd, block_size=block_size, n_head=n_head, n_layer=n_layer)\n",
    "# print the number of parameters in the model\n",
    "model.build((batch_size, block_size))\n",
    "print(\"Number of trainable parameters:\", model.count_params())\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizers.Adam(learning_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "0c038cd2-1f5e-4541-9a1c-bd4888dc7240",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"bigram_language_model_6\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"bigram_language_model_6\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)             │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ embedding_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)             │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ block_1332 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Block</span>)                   │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ block_1333 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Block</span>)                   │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ block_1334 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Block</span>)                   │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ block_1335 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Block</span>)                   │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ block_1336 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Block</span>)                   │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ block_1337 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Block</span>)                   │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ layer_normalization_2683             │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_2683 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding_14 (\u001b[38;5;33mEmbedding\u001b[0m)             │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ embedding_15 (\u001b[38;5;33mEmbedding\u001b[0m)             │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ block_1332 (\u001b[38;5;33mBlock\u001b[0m)                   │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ block_1333 (\u001b[38;5;33mBlock\u001b[0m)                   │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ block_1334 (\u001b[38;5;33mBlock\u001b[0m)                   │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ block_1335 (\u001b[38;5;33mBlock\u001b[0m)                   │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ block_1336 (\u001b[38;5;33mBlock\u001b[0m)                   │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ block_1337 (\u001b[38;5;33mBlock\u001b[0m)                   │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ layer_normalization_2683             │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_2683 (\u001b[38;5;33mDense\u001b[0m)                   │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "82361f58-4bd9-4587-afc1-1e6f73d519ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1358s\u001b[0m 13s/step - loss: 10.5332 - val_loss: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(\n",
    "    train_data_generator,\n",
    "    # epochs=Epochs,\n",
    "    epochs=1,\n",
    "    steps_per_epoch=eval_interval,\n",
    "    validation_data=val_data_generator,\n",
    "    validation_steps=eval_iters,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "c9fcc7c8-bf91-4256-ad7d-d8bfc8e0aee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[616 361 383 355]]\n"
     ]
    }
   ],
   "source": [
    "# Generate text\n",
    "# prompt = \"waati sera, wuli ka bɔ\"\n",
    "prompt = \"kuma don sera\"\n",
    "context = SuperTokenizer.encode(prompt)\n",
    "context = np.array([context], dtype=np.int64)\n",
    "# context = [context]\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "90835980-be38-48f8-b535-79c9e337cc09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " kuma don seraunsigi\n"
     ]
    }
   ],
   "source": [
    "generated = model.generate(context, max_new_tokens=30)\n",
    "print(SuperTokenizer.decode(generated[0].numpy().tolist()))\n",
    "# print()\n",
    "# generated\n",
    "# print(SuperTokenizer.decode([616, 361, 383, 355, 13]))\n",
    "# print(SuperTokenizer.encode(\"kuma don sera.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "b1b35a96-bde0-43ed-bbac-878748658b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BigramLanguageModel name=bigram_language_model_6, built=True>\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "model_name = \"E:\\\\Alkaou\\Python Projects\\\\models\\\\br_bigram_model.keras\"\n",
    "# Sauvegarder le modèle\n",
    "keras.models.save_model(model, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "4172399c-eef9-453c-a8b8-a5c76faa58ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"bigram_language_model_6\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"bigram_language_model_6\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)             │ ?                           │      <span style=\"color: #00af00; text-decoration-color: #00af00\">42,560,000</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ embedding_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)             │ ?                           │          <span style=\"color: #00af00; text-decoration-color: #00af00\">34,048</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ block_1338 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Block</span>)                   │ ?                           │      <span style=\"color: #00af00; text-decoration-color: #00af00\">13,581,948</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ block_1339 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Block</span>)                   │ ?                           │      <span style=\"color: #00af00; text-decoration-color: #00af00\">13,581,948</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ block_1340 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Block</span>)                   │ ?                           │      <span style=\"color: #00af00; text-decoration-color: #00af00\">13,581,948</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ block_1341 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Block</span>)                   │ ?                           │      <span style=\"color: #00af00; text-decoration-color: #00af00\">13,581,948</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ block_1342 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Block</span>)                   │ ?                           │      <span style=\"color: #00af00; text-decoration-color: #00af00\">13,581,948</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ block_1343 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Block</span>)                   │ ?                           │      <span style=\"color: #00af00; text-decoration-color: #00af00\">13,581,948</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ layer_normalization_2696             │ ?                           │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_2696 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                           │      <span style=\"color: #00af00; text-decoration-color: #00af00\">42,600,000</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding_16 (\u001b[38;5;33mEmbedding\u001b[0m)             │ ?                           │      \u001b[38;5;34m42,560,000\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ embedding_17 (\u001b[38;5;33mEmbedding\u001b[0m)             │ ?                           │          \u001b[38;5;34m34,048\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ block_1338 (\u001b[38;5;33mBlock\u001b[0m)                   │ ?                           │      \u001b[38;5;34m13,581,948\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ block_1339 (\u001b[38;5;33mBlock\u001b[0m)                   │ ?                           │      \u001b[38;5;34m13,581,948\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ block_1340 (\u001b[38;5;33mBlock\u001b[0m)                   │ ?                           │      \u001b[38;5;34m13,581,948\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ block_1341 (\u001b[38;5;33mBlock\u001b[0m)                   │ ?                           │      \u001b[38;5;34m13,581,948\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ block_1342 (\u001b[38;5;33mBlock\u001b[0m)                   │ ?                           │      \u001b[38;5;34m13,581,948\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ block_1343 (\u001b[38;5;33mBlock\u001b[0m)                   │ ?                           │      \u001b[38;5;34m13,581,948\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ layer_normalization_2696             │ ?                           │           \u001b[38;5;34m2,128\u001b[0m │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_2696 (\u001b[38;5;33mDense\u001b[0m)                   │ ?                           │      \u001b[38;5;34m42,600,000\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">500,063,594</span> (1.86 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m500,063,594\u001b[0m (1.86 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">166,687,864</span> (635.86 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m166,687,864\u001b[0m (635.86 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">333,375,730</span> (1.24 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m333,375,730\u001b[0m (1.24 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "md_loaded = keras.models.load_model(model_name)\n",
    "md_loaded.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "620ced4e-d6e6-4adc-8779-ad360be60dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]]\n"
     ]
    }
   ],
   "source": [
    "# Generate text\n",
    "context = np.zeros((1, 1), dtype=np.int64)\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8d323533-caf1-49ac-954c-47917ab315b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\n",
      "ɔndiya yeama ye awlenw, jukɔrɔ, f la haliɔ an ye siniɲɛsigi kad wula\n"
     ]
    }
   ],
   "source": [
    "generated = md_loaded.generate(context, max_new_tokens=20)\n",
    "print(SuperTokenizer.decode(generated[0].numpy().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cee4f9a-2079-4c57-8b42-7b8cb429b7fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
